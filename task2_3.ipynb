{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ckg03:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>assignment3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=assignment3>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appName = 'assignment3'\n",
    "master = 'local[*]'\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"INFO\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_line(line):\n",
    "    line = line.split(',')\n",
    "    return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "def avg(item):\n",
    "    vec = item[1]\n",
    "    s = 0\n",
    "    temp = {}\n",
    "    for x in vec:\n",
    "        s += float(x[1])\n",
    "        temp[x[0]] = x[1]\n",
    "        \n",
    "    return (item[0], {'vec': temp, 'avg': s/len(vec)})\n",
    "\n",
    "def read_business(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[1], (x[0], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for b, u in business_users:\n",
    "        temp[b] = u\n",
    "    return temp\n",
    "\n",
    "\n",
    "def read_user(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for u, b in business_users:\n",
    "        temp[u] = b\n",
    "    return temp\n",
    "\n",
    "\n",
    "def pearson_correlation(business1, business2):\n",
    "    vec_set1 = set()\n",
    "    b1_sum = 0\n",
    "    for user, rating in business1['vec'].items():\n",
    "        b1_sum += float(rating)\n",
    "        vec_set1.add(user)\n",
    "    \n",
    "    b2_sum = 0\n",
    "    vec_set2 = set()\n",
    "    for user, rating in business2['vec'].items():\n",
    "        b2_sum += float(rating)\n",
    "        vec_set2.add(user)\n",
    "            \n",
    "    vec_set = vec_set1.union(vec_set2) \n",
    "    if(len(vec_set) == 0):\n",
    "        return 0.0\n",
    "    b1_sum += (len(vec_set)-len(business1['vec'])) * 3.5\n",
    "    b2_sum += (len(vec_set)-len(business2['vec'])) * 3.5\n",
    "    b1_avg = b1_sum/len(vec_set)\n",
    "    b2_avg = b2_sum/len(vec_set)\n",
    "\n",
    "    corr_numerator = 0\n",
    "    corr_denominator_p1 = 0\n",
    "    corr_denominator_p2 = 0\n",
    "    \n",
    "    for user_id in vec_set:\n",
    "        v1 = None\n",
    "        if(user_id in business1['vec']):\n",
    "            v1 = float(business1['vec'][user_id])-b1_avg\n",
    "        else:\n",
    "            v1 = business1['avg']-b1_avg\n",
    "            \n",
    "            \n",
    "        if(user_id in business2['vec']):\n",
    "            v2 = float(business2['vec'][user_id])-b2_avg\n",
    "        else:\n",
    "            v2 = business2['avg']-b2_avg\n",
    "        \n",
    "        corr_numerator += v1*v2\n",
    "        corr_denominator_p1 += math.pow(v1, 2)\n",
    "        corr_denominator_p2 += math.pow(v2, 2)\n",
    "    \n",
    "    denominator = math.pow(corr_denominator_p1, 0.5) * math.pow(corr_denominator_p2, 0.5)\n",
    "    \n",
    "    if(denominator != 0):\n",
    "        return corr_numerator / denominator\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "def calculate_rating_partition(train_data, user_avg, test_data):\n",
    "    \n",
    "    for (user_id, business_id) in test_data:\n",
    "    \n",
    "        if(not business_id in train_data and not user_id in user_avg):\n",
    "            yield (user_id, business_id, 4.0)\n",
    "            continue\n",
    "        if(not business_id in train_data and user_id in user_avg):\n",
    "            yield (user_id, business_id, user_avg[user_id]['avg'])\n",
    "            continue\n",
    "        if(business_id in train_data and not user_id in user_avg):\n",
    "            yield (user_id, business_id, train_data[business_id]['avg'])\n",
    "            continue\n",
    "\n",
    "        business_data = train_data[business_id]\n",
    "        similar_items = []\n",
    "        for bid, rating in user_avg[user_id]['vec'].items():\n",
    "            b = train_data[bid]\n",
    "            sim = pearson_correlation(business_data, b)\n",
    "            if(sim > 0.0):\n",
    "                similar_items.append((sim, float(b['vec'][user_id])))\n",
    "    #     similar_item = sorted(similar_items, key=lambda x: x[0], reverse=True)\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for similarity, rating in similar_items:\n",
    "            weight = similarity# * math.pow(abs(similarity), rho-1)\n",
    "            numerator += weight*rating\n",
    "            denominator += weight\n",
    "\n",
    "        if(denominator != 0):\n",
    "            yield (user_id, business_id, numerator/denominator)\n",
    "            \n",
    "        else:\n",
    "            yield (user_id, business_id, train_data[business_id]['avg'])\n",
    "            \n",
    "    \n",
    "    \n",
    "def save_output(output, path):\n",
    "    output_file = open(path, 'wt')\n",
    "    output_file.write('user_id, business_id, prediction\\n')\n",
    "    for line in output:\n",
    "        output_file.write(line)\n",
    "    output_file.close()\n",
    "    return\n",
    "    \n",
    "def read_test_data(test_path):\n",
    "    def read_test_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip())\n",
    "    \n",
    "    rdd = sc.textFile(test_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_test_line)    \n",
    "    return rdd\n",
    "\n",
    "\n",
    "def read_train_data(train_path):\n",
    "    def read_csv_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "    \n",
    "    rdd = sc.textFile(train_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    return rdd\n",
    "\n",
    "\n",
    "\n",
    "def rmse(test_path, pred_path):\n",
    "    test_data = {}\n",
    "    count = 0\n",
    "    with open(test_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            count+=1\n",
    "            test_data[tuple(line.split(',')[:2])] = float(line.split(',')[2])\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    sum_error_square = 0\n",
    "    with open(pred_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            if(line == 'Null'):\n",
    "                line = file.readline().strip()\n",
    "                continue\n",
    "            sum_error_square+=math.pow((test_data[tuple(line.split(',')[:2])]-float(line.split(',')[2])), 2)\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    return math.pow(sum_error_square/count, 0.5)\n",
    "\n",
    "def model_based_recommendation(folder_path, test_path):\n",
    "    \n",
    "    business_data = sc.textFile(os.path.join(folder_path, 'business.json')).map(json.loads).map(lambda x: (x['business_id'], x['review_count'], x['stars'])).collect()\n",
    "    user_data = sc.textFile(os.path.join(folder_path, 'user.json')).map(json.loads).map(lambda x: (x['user_id'], x['review_count'], x['average_stars'])).collect()\n",
    "    checkin_data = sc.textFile(os.path.join(folder_path, 'checkin.json')).map(json.loads).map(lambda x: (x['business_id'], sum(x['time'].values()))).collect()\n",
    "    test_data = read_test_data(test_path).collect()\n",
    "    train_data = read_train_data(os.path.join(folder_path, 'yelp_train.csv')).collect()\n",
    "    business_data = pd.DataFrame(business_data, columns=['business_id', 'review_count', 'stars'])\n",
    "    user_data = pd.DataFrame(user_data, columns=['user_id', 'review_count', 'average_stars'])\n",
    "    checkin_data = pd.DataFrame(checkin_data, columns=['business_id', 'total_checkins'])\n",
    "    train_data = pd.DataFrame(train_data, columns=['user_id', 'business_id', 'rating'])\n",
    "    test_data = pd.DataFrame(test_data, columns=['user_id', 'business_id'])\n",
    "\n",
    "\n",
    "\n",
    "    business_data = pd.merge(business_data, checkin_data, on='business_id', how='left')\n",
    "    train_data = pd.merge(train_data, user_data, on='user_id', how='left')\n",
    "    train_data = pd.merge(train_data, business_data, on='business_id', how='left')\n",
    "    test_data = pd.merge(test_data, user_data, on='user_id', how='left')\n",
    "    test_data = pd.merge(test_data, business_data, on='business_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    train_x = train_data.loc[:, ('review_count_x', 'average_stars', 'review_count_y', 'stars', 'total_checkins')].values\n",
    "    train_y = train_data.loc[:, ('rating')].values\n",
    "    test_x = test_data.loc[:, ('review_count_x', 'average_stars', 'review_count_y', 'stars', 'total_checkins')].values\n",
    "#     test_y = test_data.loc[:, ('rating')].values\n",
    "\n",
    "\n",
    "\n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    output = pd.concat((test_data.loc[:, ('user_id', 'business_id')], pd.DataFrame(pred, columns=['pred_model'])), axis=1)\n",
    "    output.pred_model = output.pred_model.apply(lambda x: '5.0' if x>5 else str(x))\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def item_based_recommendation(train_path, test_path):\n",
    "    st = time.time()\n",
    "    \n",
    "    train_data = read_business(train_path)\n",
    "    test_data = read_test_data(test_path).collect()\n",
    "    user_avg = read_user(train_path)\n",
    "\n",
    "    output = sc.parallelize(test_data).mapPartitions(lambda x: calculate_rating_partition(train_data, user_avg, x)).collect()\n",
    "    print(time.time()-st)\n",
    "    output = pd.DataFrame(output, columns=['user_id', 'business_id', 'pred_item'])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = sys.argv[1].strip()\n",
    "# test_path = sys.argv[2].strip()\n",
    "# output_path = sys.argv[3].strip()\n",
    "\n",
    "folder_path = './data'\n",
    "train_path = os.path.join(folder_path, 'yelp_train.csv')\n",
    "test_path = './data/yelp_val.csv'\n",
    "output_path = './output2_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_based = model_based_recommendation(folder_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>pred_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>3.770955801010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.876779079437256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.907809257507324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.6978631019592285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.9433350563049316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142039</td>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.410508632659912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142040</td>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>2.9661436080932617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142041</td>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.720184564590454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142042</td>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>4.030874252319336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142043</td>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.39371395111084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id          pred_model\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg   3.770955801010132\n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ   4.876779079437256\n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA   4.907809257507324\n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA  4.6978631019592285\n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw  2.9433350563049316\n",
       "...                        ...                     ...                 ...\n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q   3.410508632659912\n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA  2.9661436080932617\n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg   3.720184564590454\n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA   4.030874252319336\n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    3.39371395111084\n",
       "\n",
       "[142044 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.990034341812134\n"
     ]
    }
   ],
   "source": [
    "output_item_based = item_based_recommendation(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>pred_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>4.536926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.664881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.196924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.867370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.347667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142039</td>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.959585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142040</td>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.894218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142041</td>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.707472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142042</td>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.943722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142043</td>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.938709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  pred_item\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg   4.536926\n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ   4.664881\n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA   4.196924\n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA   4.867370\n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw   2.347667\n",
       "...                        ...                     ...        ...\n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q   3.959585\n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA   3.894218\n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg   3.707472\n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA   3.943722\n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg   3.938709\n",
       "\n",
       "[142044 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_item_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.merge(output_item_based, output_model_based, on=['user_id', 'business_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>pred_item</th>\n",
       "      <th>pred_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>4.536926</td>\n",
       "      <td>3.770955801010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.664881</td>\n",
       "      <td>4.876779079437256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.196924</td>\n",
       "      <td>4.907809257507324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.867370</td>\n",
       "      <td>4.6978631019592285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.347667</td>\n",
       "      <td>2.9433350563049316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142039</td>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.959585</td>\n",
       "      <td>3.410508632659912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142040</td>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.894218</td>\n",
       "      <td>2.9661436080932617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142041</td>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.707472</td>\n",
       "      <td>3.720184564590454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142042</td>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.943722</td>\n",
       "      <td>4.030874252319336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142043</td>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.938709</td>\n",
       "      <td>3.39371395111084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  pred_item  \\\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg   4.536926   \n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ   4.664881   \n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA   4.196924   \n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA   4.867370   \n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw   2.347667   \n",
       "...                        ...                     ...        ...   \n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q   3.959585   \n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA   3.894218   \n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg   3.707472   \n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA   3.943722   \n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg   3.938709   \n",
       "\n",
       "                pred_model  \n",
       "0        3.770955801010132  \n",
       "1        4.876779079437256  \n",
       "2        4.907809257507324  \n",
       "3       4.6978631019592285  \n",
       "4       2.9433350563049316  \n",
       "...                    ...  \n",
       "142039   3.410508632659912  \n",
       "142040  2.9661436080932617  \n",
       "142041   3.720184564590454  \n",
       "142042   4.030874252319336  \n",
       "142043    3.39371395111084  \n",
       "\n",
       "[142044 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835699834473771"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pred'] = output.apply(lambda row: str(1*float(row.pred_model)+0*float(row.pred_item)), axis=1)\n",
    "output = output.loc[:, ('user_id', 'business_id', 'pred')].apply(lambda row: ','.join(row)+'\\n', axis=1).tolist()\n",
    "save_output(output, output_path)\n",
    "rmse(test_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "\n",
    "appName = 'assignment3'\n",
    "master = 'local[*]'\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def lsh(train_data_path):\n",
    "    # Total number of users 11270\n",
    "    p = 13591\n",
    "    m = 11270\n",
    "    number_of_hashes = 128\n",
    "    random.seed(123)\n",
    "    a = random.sample(range(1, p), number_of_hashes)\n",
    "    b = random.sample(range(0, p), number_of_hashes)\n",
    "\n",
    "\n",
    "    def read_csv_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "    def read_business(data_path):\n",
    "        rdd = sc.textFile(data_path)\n",
    "        header = rdd.first()\n",
    "        rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "        unique_users = rdd.map(lambda x: x[0]).distinct().collect()    \n",
    "        userid_index = {}\n",
    "        for i, uid in enumerate(unique_users):\n",
    "            userid_index[uid] = i\n",
    "\n",
    "        business_users = rdd.map(lambda x: (x[1], userid_index[x[0]])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "        return business_users\n",
    "\n",
    "    def calculate_hash(a, b, p, m, vec):\n",
    "        return min([((a*x)+b)%p%m for x in vec])\n",
    "\n",
    "    def minhash(vector, a, b, p, m):\n",
    "        return [calculate_hash(ai, bi, p, m, vector) for ai, bi in zip(a, b)]\n",
    "    #     return list(np.min((a.reshape(1, a.shape[0]) * vector.reshape(vector.shape[0], 1) + b) %p %m, axis=0))\n",
    "\n",
    "    def hash_bands(x):\n",
    "        doc_id, sig = x\n",
    "        b = 64\n",
    "        r = 2\n",
    "        output = []\n",
    "        for i in range(0, b):\n",
    "            output.append(((i, hash(tuple(sig[r*i:(i+1)*r]))), doc_id))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def jaccard_sim(x, y):\n",
    "        x = set(x)\n",
    "        y = set(y)\n",
    "        return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "    def index_signatures(signatures):\n",
    "        index = {}\n",
    "        for x, y in signatures:\n",
    "            index[x] = y\n",
    "\n",
    "        return index\n",
    "    \n",
    "    \n",
    "    rdd = read_business(train_data_path)\n",
    "    signatures = rdd.collect()\n",
    "    index = index_signatures(signatures)\n",
    "    business_sig = sc.parallelize(signatures).map(lambda x: (x[0], minhash(x[1], a, b, p, m)))\n",
    "    candidates = business_sig.flatMap(hash_bands).groupByKey().map(lambda x: (x[0], list(x[1]))).filter(lambda x: len(x[1]) > 1).collect()\n",
    "    reverse_index = {}\n",
    "    candidates_index = {}\n",
    "\n",
    "    for bid, c in candidates:\n",
    "        candidates_index[bid] = c\n",
    "\n",
    "    for bucket_id, bucket in candidates:\n",
    "        for bid in bucket:\n",
    "            reverse_index[bid] = bucket_id\n",
    "            \n",
    "    return candidates_index, reverse_index\n",
    "\n",
    "\n",
    "\n",
    "def read_csv_line(line):\n",
    "    line = line.split(',')\n",
    "    return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "def avg(item):\n",
    "    vec = item[1]\n",
    "    s = 0\n",
    "    temp = {}\n",
    "    for x in vec:\n",
    "        s += float(x[1])\n",
    "        temp[x[0]] = x[1]\n",
    "        \n",
    "    return (item[0], {'vec': temp, 'avg': s/len(vec)})\n",
    "\n",
    "def read_business(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[1], (x[0], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for b, u in business_users:\n",
    "        temp[b] = u\n",
    "    return temp\n",
    "\n",
    "\n",
    "def read_user(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for u, b in business_users:\n",
    "        temp[u] = b\n",
    "    return temp\n",
    "\n",
    "\n",
    "def pearson_correlation(business1, business2):\n",
    "    vec_set1 = set()\n",
    "    b1_sum = 0\n",
    "    for user, rating in business1['vec'].items():\n",
    "        b1_sum += float(rating)\n",
    "        vec_set1.add(user)\n",
    "    \n",
    "    b2_sum = 0\n",
    "    vec_set2 = set()\n",
    "    for user, rating in business2['vec'].items():\n",
    "        b2_sum += float(rating)\n",
    "        vec_set2.add(user)\n",
    "            \n",
    "    vec_set = vec_set1.union(vec_set2) \n",
    "    if(len(vec_set) == 0):\n",
    "        return 0.0\n",
    "    b1_sum += (len(vec_set)-len(business1['vec'])) * 3.5\n",
    "    b2_sum += (len(vec_set)-len(business2['vec'])) * 3.5\n",
    "    b1_avg = b1_sum/len(vec_set)\n",
    "    b2_avg = b2_sum/len(vec_set)\n",
    "\n",
    "    corr_numerator = 0\n",
    "    corr_denominator_p1 = 0\n",
    "    corr_denominator_p2 = 0\n",
    "    \n",
    "    for user_id in vec_set:\n",
    "        v1 = None\n",
    "        if(user_id in business1['vec']):\n",
    "            v1 = float(business1['vec'][user_id])-b1_avg\n",
    "        else:\n",
    "            v1 = business1['avg']-b1_avg\n",
    "            \n",
    "            \n",
    "        if(user_id in business2['vec']):\n",
    "            v2 = float(business2['vec'][user_id])-b2_avg\n",
    "        else:\n",
    "            v2 = business2['avg']-b2_avg\n",
    "        \n",
    "        corr_numerator += v1*v2\n",
    "        corr_denominator_p1 += math.pow(v1, 2)\n",
    "        corr_denominator_p2 += math.pow(v2, 2)\n",
    "    \n",
    "    denominator = math.pow(corr_denominator_p1, 0.5) * math.pow(corr_denominator_p2, 0.5)\n",
    "    \n",
    "    if(denominator != 0):\n",
    "        return corr_numerator / denominator\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "def calculate_rating(train_data, user_avg, reverse_index, candidates_index, business_id, user_id):\n",
    "    if(not business_id in train_data and not user_id in user_avg):\n",
    "        return (user_id, business_id, 4.0)\n",
    "    if(not business_id in train_data and user_id in user_avg):\n",
    "        return (user_id, business_id, user_avg[user_id]['avg'])\n",
    "    if(business_id in train_data and not user_id in user_avg):\n",
    "        return (user_id, business_id, train_data[business_id]['avg'])\n",
    "    \n",
    "    business_data = train_data[business_id]\n",
    "    similar_items = []\n",
    "    for bid in candidates_index[reverse_index['FaHADZARwnY4yvlvpnsfGA']]:\n",
    "        b = train_data[bid]\n",
    "        if(not user_id in b['vec']):\n",
    "            continue\n",
    "        sim = pearson_correlation(business_data, b)\n",
    "        if(sim > 0.0):\n",
    "            similar_items.append((sim, float(b['vec'][user_id])))\n",
    "#     similar_item = sorted(similar_items, key=lambda x: x[0], reverse=True)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for similarity, rating in similar_items:\n",
    "        weight = similarity# * math.pow(abs(similarity), rho-1)\n",
    "        numerator += weight*rating\n",
    "        denominator += weight\n",
    "        \n",
    "    if(denominator != 0):\n",
    "        return (user_id, business_id, numerator/denominator)\n",
    "    else:\n",
    "        return (user_id, business_id, train_data[business_id]['avg'])\n",
    "\n",
    "    \n",
    "    \n",
    "def save_output(output, path):\n",
    "    output_file = open(path, 'wt')\n",
    "    output_file.write('user_id, business_id, prediction\\n')\n",
    "    for line in output:\n",
    "        output_file.write(line)\n",
    "    output_file.close()\n",
    "    return\n",
    "    \n",
    "def read_test_data(test_path):\n",
    "    def read_test_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip())\n",
    "    \n",
    "    rdd = sc.textFile(test_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_test_line)    \n",
    "    return rdd\n",
    "\n",
    "\n",
    "def read_train_data(train_path):\n",
    "    def read_csv_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "    \n",
    "    rdd = sc.textFile(train_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    return rdd\n",
    "\n",
    "\n",
    "\n",
    "def rmse(test_path, pred_path):\n",
    "    test_data = {}\n",
    "    count = 0\n",
    "    with open(test_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            count+=1\n",
    "            test_data[tuple(line.split(',')[:2])] = float(line.split(',')[2])\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    sum_error_square = 0\n",
    "    with open(pred_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            if(line == 'Null'):\n",
    "                line = file.readline().strip()\n",
    "                continue\n",
    "            sum_error_square+=math.pow((test_data[tuple(line.split(',')[:2])]-float(line.split(',')[2])), 2)\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    return math.pow(sum_error_square/count, 0.5)\n",
    "\n",
    "def model_based_recommendation(folder_path, test_path):\n",
    "    \n",
    "    business_data = sc.textFile(os.path.join(folder_path, 'business.json')).map(json.loads).map(lambda x: (x['business_id'], x['review_count'], x['stars'])).collect()\n",
    "    user_data = sc.textFile(os.path.join(folder_path, 'user.json')).map(json.loads).map(lambda x: (x['user_id'], x['review_count'], x['average_stars'])).collect()\n",
    "    checkin_data = sc.textFile(os.path.join(folder_path, 'checkin.json')).map(json.loads).map(lambda x: (x['business_id'], sum(x['time'].values()))).collect()\n",
    "    test_data = read_test_data(test_path).collect()\n",
    "    train_data = read_train_data(os.path.join(folder_path, 'yelp_train.csv')).collect()\n",
    "    business_data = pd.DataFrame(business_data, columns=['business_id', 'review_count', 'stars'])\n",
    "    user_data = pd.DataFrame(user_data, columns=['user_id', 'review_count', 'average_stars'])\n",
    "    checkin_data = pd.DataFrame(checkin_data, columns=['business_id', 'total_checkins'])\n",
    "    train_data = pd.DataFrame(train_data, columns=['user_id', 'business_id', 'rating'])\n",
    "    test_data = pd.DataFrame(test_data, columns=['user_id', 'business_id'])\n",
    "\n",
    "\n",
    "\n",
    "    business_data = pd.merge(business_data, checkin_data, on='business_id', how='left')\n",
    "    train_data = pd.merge(train_data, user_data, on='user_id', how='left')\n",
    "    train_data = pd.merge(train_data, business_data, on='business_id', how='left')\n",
    "    test_data = pd.merge(test_data, user_data, on='user_id', how='left')\n",
    "    test_data = pd.merge(test_data, business_data, on='business_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    train_x = train_data.loc[:, ('review_count_x', 'average_stars', 'review_count_y', 'stars', 'total_checkins')].values\n",
    "    train_y = train_data.loc[:, ('rating')].values\n",
    "    test_x = test_data.loc[:, ('review_count_x', 'average_stars', 'review_count_y', 'stars', 'total_checkins')].values\n",
    "#     test_y = test_data.loc[:, ('rating')].values\n",
    "\n",
    "\n",
    "\n",
    "    model = xgb.XGBRegressor(objective ='reg:linear')\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    output = pd.concat((test_data.loc[:, ('user_id', 'business_id')], pd.DataFrame(pred, columns=['pred_model'])), axis=1)\n",
    "    output.pred_model = output.pred_model.apply(lambda x: '5.0' if x>5 else str(x))\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def item_based_recommendation(train_path, test_path):\n",
    "    candidates_index, reverse_index = lsh(train_path)\n",
    "    train_data = read_business(train_path)\n",
    "    test_data = read_test_data(test_path).collect()\n",
    "    user_avg = read_user(train_path)\n",
    "\n",
    "    output = sc.parallelize(test_data).map(lambda x: calculate_rating(train_data, user_avg, reverse_index, candidates_index, x[0], x[1])).collect()\n",
    "    \n",
    "    output = pd.DataFrame(output, columns=['user_id', 'business_id', 'pred_item'])\n",
    "    return output\n",
    "\n",
    "def combine_predictions(p1, p2):\n",
    "#     if(p1 == None and p2 == None):\n",
    "#         return '3.5'\n",
    "#     if(p2 == None):\n",
    "#         return p1\n",
    "#     if(p1 == None):\n",
    "#         return p2\n",
    "    \n",
    "    return str(0.9*float(p1)+0.1*float(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:58:03] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "folder_path = './data'\n",
    "train_path = os.path.join(folder_path, 'yelp_train.csv')\n",
    "test_path = './data/yelp_val.csv'\n",
    "output_path = './output2_3.csv'\n",
    "\n",
    "st = time.time()\n",
    "output_model_based = model_based_recommendation(folder_path, test_path)\n",
    "output_item_based = item_based_recommendation(train_path, test_path)\n",
    "output = pd.merge(output_model_based, output_item_based, on=['user_id', 'business_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>pred_model</th>\n",
       "      <th>pred_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, business_id, pred_model, pred_item]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(output_model_based, output_item_based, on=['user_id', 'business_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>pred_model</th>\n",
       "      <th>pred_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, business_id, pred_model, pred_item]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.apply(lambda row: combine_predictions(row.pred_model, row.pred_item), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.apply(lambda row: combine_predictions(row.pred_model, row.pred_item), axis=0)\n",
    "print(output)\n",
    "\n",
    "output = output.loc[:, ('user_id', 'business_id', 'pred')].apply(lambda row: ','.join(row)+'\\n', axis=1).tolist()\n",
    "print(output[:20])\n",
    "save_output(output, output_path)\n",
    "print(time.time()-st)\n",
    "print(rmse(test_path, output_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
