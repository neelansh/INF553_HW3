{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ckg03:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>assignment3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=assignment3>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appName = 'assignment3'\n",
    "master = 'local[*]'\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lsh(train_data_path):\n",
    "    # Total number of users 11270\n",
    "    p = 13591\n",
    "    m = 11270\n",
    "    number_of_hashes = 128\n",
    "    a = random.sample(range(1, p), number_of_hashes)\n",
    "    b = random.sample(range(0, p), number_of_hashes)\n",
    "\n",
    "\n",
    "    def read_csv_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "    def read_business(data_path):\n",
    "        rdd = sc.textFile(data_path)\n",
    "        header = rdd.first()\n",
    "        rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "        unique_users = rdd.map(lambda x: x[0]).distinct().collect()    \n",
    "        userid_index = {}\n",
    "        for i, uid in enumerate(unique_users):\n",
    "            userid_index[uid] = i\n",
    "\n",
    "        business_users = rdd.map(lambda x: (x[1], userid_index[x[0]])).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "        return business_users\n",
    "\n",
    "    def calculate_hash(a, b, p, m, vec):\n",
    "        return min([((a*x)+b)%p%m for x in vec])\n",
    "\n",
    "    def minhash(vector, a, b, p, m):\n",
    "        return [calculate_hash(ai, bi, p, m, vector) for ai, bi in zip(a, b)]\n",
    "    #     return list(np.min((a.reshape(1, a.shape[0]) * vector.reshape(vector.shape[0], 1) + b) %p %m, axis=0))\n",
    "\n",
    "    def hash_bands(x):\n",
    "        doc_id, sig = x\n",
    "        b = 64\n",
    "        r = 2\n",
    "        output = []\n",
    "        for i in range(0, b):\n",
    "            output.append(((i, hash(tuple(sig[r*i:(i+1)*r]))), doc_id))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def jaccard_sim(x, y):\n",
    "        x = set(x)\n",
    "        y = set(y)\n",
    "        return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "    def index_signatures(signatures):\n",
    "        index = {}\n",
    "        for x, y in signatures:\n",
    "            index[x] = y\n",
    "\n",
    "        return index\n",
    "    \n",
    "    \n",
    "    rdd = read_business(train_data_path)\n",
    "    signatures = rdd.collect()\n",
    "    index = index_signatures(signatures)\n",
    "    business_sig = sc.parallelize(signatures).map(lambda x: (x[0], minhash(x[1], a, b, p, m)))\n",
    "    candidates = business_sig.flatMap(hash_bands).groupByKey().map(lambda x: (x[0], list(x[1]))).filter(lambda x: len(x[1]) > 1).collect()\n",
    "    reverse_index = {}\n",
    "    candidates_index = {}\n",
    "\n",
    "    for bid, c in candidates:\n",
    "        candidates_index[bid] = c\n",
    "\n",
    "    for bucket_id, bucket in candidates:\n",
    "        for bid in bucket:\n",
    "            reverse_index[bid] = bucket_id\n",
    "            \n",
    "    return candidates_index, reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_index, reverse_index = lsh('./data/yelp_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143615"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_line(line):\n",
    "    line = line.split(',')\n",
    "    return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "def avg(item):\n",
    "    vec = item[1]\n",
    "    s = 0\n",
    "    temp = {}\n",
    "    for x in vec:\n",
    "        s += float(x[1])\n",
    "        temp[x[0]] = x[1]\n",
    "        \n",
    "    return (item[0], {'vec': temp, 'avg': s/len(vec)})\n",
    "\n",
    "def read_business(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[1], (x[0], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for b, u in business_users:\n",
    "        temp[b] = u\n",
    "    return temp\n",
    "\n",
    "\n",
    "def read_user(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for u, b in business_users:\n",
    "        temp[u] = b\n",
    "    return temp\n",
    "\n",
    "\n",
    "def pearson_correlation(business1, business2):\n",
    "    vec_set1 = set()\n",
    "    b1_sum = 0\n",
    "    for user, rating in business1['vec'].items():\n",
    "        b1_sum += float(rating)\n",
    "        vec_set1.add(user)\n",
    "    \n",
    "    b2_sum = 0\n",
    "    vec_set2 = set()\n",
    "    for user, rating in business2['vec'].items():\n",
    "        b2_sum += float(rating)\n",
    "        vec_set2.add(user)\n",
    "            \n",
    "    vec_set = vec_set1.union(vec_set2) \n",
    "    if(len(vec_set) == 0):\n",
    "        return 0.0\n",
    "    b1_sum += (len(vec_set)-len(business1['vec'])) * 3.5\n",
    "    b2_sum += (len(vec_set)-len(business2['vec'])) * 3.5\n",
    "    b1_avg = b1_sum/len(vec_set)\n",
    "    b2_avg = b2_sum/len(vec_set)\n",
    "\n",
    "    corr_numerator = 0\n",
    "    corr_denominator_p1 = 0\n",
    "    corr_denominator_p2 = 0\n",
    "    \n",
    "    for user_id in vec_set:\n",
    "        v1 = None\n",
    "        if(user_id in business1['vec']):\n",
    "            v1 = float(business1['vec'][user_id])-b1_avg\n",
    "        else:\n",
    "            v1 = business1['avg']-b1_avg\n",
    "            \n",
    "            \n",
    "        if(user_id in business2['vec']):\n",
    "            v2 = float(business2['vec'][user_id])-b2_avg\n",
    "        else:\n",
    "            v2 = business2['avg']-b2_avg\n",
    "        \n",
    "        corr_numerator += v1*v2\n",
    "        corr_denominator_p1 += math.pow(v1, 2)\n",
    "        corr_denominator_p2 += math.pow(v2, 2)\n",
    "    \n",
    "    denominator = math.pow(corr_denominator_p1, 0.5) * math.pow(corr_denominator_p2, 0.5)\n",
    "    \n",
    "    if(denominator != 0):\n",
    "        return corr_numerator / denominator\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rating(train_data, user_avg, reverse_index, candidates_index, business_id, user_id):\n",
    "    if(not business_id in train_data and not user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, 4.0)\n",
    "    if(not business_id in train_data and user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, user_avg[user_id]['avg'])\n",
    "    if(business_id in train_data and not user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "    \n",
    "    business_data = train_data[business_id]\n",
    "    similar_items = []\n",
    "    for bid in candidates_index[reverse_index['FaHADZARwnY4yvlvpnsfGA']]:\n",
    "        b = train_data[bid]\n",
    "        if(not user_id in b['vec']):\n",
    "            continue\n",
    "        sim = pearson_correlation(business_data, b)\n",
    "        if(sim > 0.0):\n",
    "            similar_items.append((sim, float(b['vec'][user_id])))\n",
    "#     similar_item = sorted(similar_items, key=lambda x: x[0], reverse=True)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for similarity, rating in similar_items:\n",
    "        weight = similarity# * math.pow(abs(similarity), rho-1)\n",
    "        numerator += weight*rating\n",
    "        denominator += weight\n",
    "        \n",
    "    if(denominator != 0):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, numerator/denominator)\n",
    "    else:\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "\n",
    "    \n",
    "    \n",
    "# def calculate_rating_partition(train_data, user_avg, test_data):\n",
    "    \n",
    "#     for (business_id, user_id) in test_data:\n",
    "    \n",
    "#         if(not business_id in train_data and not user_id in user_avg):\n",
    "#             yield \"{},{},{}\\n\".format(user_id, business_id, 4.0)\n",
    "#             continue\n",
    "#         if(not business_id in train_data and user_id in user_avg):\n",
    "#             yield \"{},{},{}\\n\".format(user_id, business_id, user_avg[user_id]['avg'])\n",
    "#             continue\n",
    "#         if(business_id in train_data and not user_id in user_avg):\n",
    "#             yield \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "#             continue\n",
    "\n",
    "#         business_data = train_data[business_id]\n",
    "#         similar_items = []\n",
    "#         for bid, rating in user_avg[user_id]['vec'].items():\n",
    "#             b = train_data[bid]\n",
    "#             sim = pearson_correlation(business_data, b)\n",
    "#             if(sim > 0.0):\n",
    "#                 similar_items.append((sim, float(b['vec'][user_id])))\n",
    "#     #     similar_item = sorted(similar_items, key=lambda x: x[0], reverse=True)\n",
    "#         numerator = 0\n",
    "#         denominator = 0\n",
    "\n",
    "#         for similarity, rating in similar_items:\n",
    "#             weight = similarity# * math.pow(abs(similarity), rho-1)\n",
    "#             numerator += weight*rating\n",
    "#             denominator += weight\n",
    "\n",
    "#         if(denominator != 0):\n",
    "#             yield \"{},{},{}\\n\".format(user_id, business_id, numerator/denominator)\n",
    "            \n",
    "#         else:\n",
    "#             yield \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "            \n",
    "    \n",
    "    \n",
    "def save_output(output, path):\n",
    "    output_file = open(path, 'wt')\n",
    "    output_file.write('user_id, business_id, prediction\\n')\n",
    "    for line in output:\n",
    "        output_file.write(line)\n",
    "    output_file.close()\n",
    "    return\n",
    "    \n",
    "def read_test_data(test_path):\n",
    "    def read_test_line(line):\n",
    "        line = line.split(',')\n",
    "        return (line[0].strip(), line[1].strip())\n",
    "    \n",
    "    rdd = sc.textFile(test_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_test_line).map(lambda x: (x[1], x[0]))#.join(train_data).map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "    \n",
    "    return rdd.collect()\n",
    "\n",
    "\n",
    "\n",
    "def rmse(test_path, pred_path):\n",
    "    test_data = {}\n",
    "    count = 0\n",
    "    with open(test_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            count+=1\n",
    "            test_data[tuple(line.split(',')[:2])] = float(line.split(',')[2])\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    sum_error_square = 0\n",
    "    with open(pred_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            if(line == 'Null'):\n",
    "                line = file.readline().strip()\n",
    "                continue\n",
    "            sum_error_square+=math.pow((test_data[tuple(line.split(',')[:2])]-float(line.split(',')[2])), 2)\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    return math.pow(sum_error_square/count, 0.5)\n",
    "\n",
    "\n",
    "# def expand(train_data, user_avg, user_id, business_id):\n",
    "#     if(not user_id in user_avg or not business_id in train_data):\n",
    "#         return []\n",
    "#     output = []\n",
    "#     for bid, r in user_avg[user_id]['vec'].items():\n",
    "#         output.append(((business_id, user_id), (bid, r)))\n",
    "        \n",
    "#     return output\n",
    "\n",
    "# def pearson_wrapper(train_data, bid1, bid2, bucket_index, candidate_index):\n",
    "#     return pearson_correlation(train_data[bid1], train_data[bid2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4780943393707275\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "train_data = read_business('./data/yelp_train.csv')\n",
    "test_data = read_test_data('./data/yelp_val.csv')\n",
    "user_avg = read_user('./data/yelp_train.csv')\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.764833927154541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0787236408418601"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "output = sc.parallelize(test_data).map(lambda x: calculate_rating(train_data, user_avg, reverse_index, candidates_index, x[0], x[1])).collect()\n",
    "print(time.time()-st)\n",
    "save_output(output, 'output2_1.csv')\n",
    "rmse('./data/yelp_val.csv', 'output2_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.18922662734985\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "def predict(x):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for n,d in x:\n",
    "        numerator += n\n",
    "        denominator += d\n",
    "    if(denominator == 0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return numerator/denominator\n",
    "output = sc.parallelize(test_data).flatMap(lambda x: expand(train_data, user_avg, x[1], x[0])).map(lambda x: (x[0], (pearson_wrapper(train_data, x[0][0], x[1][0]), x[1][1]))).filter(lambda x: x[1][0]>0).map(lambda x: (x[0], (x[1][0]*float(x[1][1]), x[1][0]))).groupByKey().map(lambda x: (x[0], predict(x[1]))).collect()\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wf1GqnKQuvH-V3QN80UOOQ,fThrN4tfupIGetkrz18JOg,3.75\\n',\n",
       " '39FT2Ui8KUXwmUt6hnwy-g,uW6UHfONAmm8QttPkbMewQ,4.403292181069959\\n',\n",
       " '7weuSPSSqYLUFga6IYP4pg,IhNASEZ3XnBHmuuVnWdIwA,4.835106382978723\\n',\n",
       " 'CqaIzLiWaa-lMFYBAsYQxw,G859H6xfAmVLxbzQgipuoA,4.2\\n',\n",
       " 'yy7shAsNWRbGg-8Y67Dzag,rS39YnrhoXmPqHLzCBjeqw,3.090909090909091\\n',\n",
       " 'Uk1UKBIAwOqhjZdLm3r9zg,5CJL_2-XwCGBmOav4mFdYg,3.5208333333333335\\n',\n",
       " 'x-8ZMKKNycT3782Kqf9loA,jgtWfJCJZty_Nctqpdtp3g,4.0125\\n',\n",
       " '0FVcoJko1kfZCrJRfssfIA,JVK8szNDoy9MNiYSz_MiAA,3.5714285714285716\\n',\n",
       " 'LcCRMIDz1JgshpPGYfLDcA,t19vb_4ML2dg5HZ-MF3muA,3.380952380952381\\n',\n",
       " 'C__1BHWTGBNA5s2ZPH289g,h_UvnQfe1cuVICly_kIqHg,3.642857142857143\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output(output, 'output2_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0794448834300172"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse('./data/yelp_val.csv', 'output2_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: generate the pairs ((bid_in, uid_in), (bid1, rating for uid))(all the bids in user vector)\n",
    "# step2: calculate similarity using pearson\n",
    "# step3: groupby first2 and \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(sys.argv[2].strip(), sys.argv[3].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_line(line):\n",
    "    line = line.split(',')\n",
    "    return (line[0].strip(), line[1].strip(), line[2].strip())\n",
    "\n",
    "def avg(item):\n",
    "    vec = item[1]\n",
    "    s = 0\n",
    "    temp = {}\n",
    "    for x in vec:\n",
    "        s += float(x[1])\n",
    "        temp[x[0]] = x[1]\n",
    "        \n",
    "    return (item[0], {'vec': temp, 'avg': s/len(vec)})\n",
    "\n",
    "def read_business(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[1], (x[0], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for b, u in business_users:\n",
    "        temp[b] = u\n",
    "    return temp\n",
    "\n",
    "\n",
    "def read_user(data_path):\n",
    "    rdd = sc.textFile(data_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line)\n",
    "    \n",
    "    business_users = rdd.map(lambda x: (x[0], (x[1], x[2]))).groupByKey().map(avg).collect()\n",
    "    temp = {}\n",
    "    for u, b in business_users:\n",
    "        temp[u] = b\n",
    "    return temp\n",
    "    \n",
    "def is_rated(x, user_y):\n",
    "    for (user, _) in x:\n",
    "        if(user == user_y):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def stage1(x, user_y):\n",
    "    for (user, rating) in x[2]['vec']:\n",
    "        if(user == user_y):\n",
    "            return [(float(rating)*x[1], x[1])]\n",
    "        \n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_business('./data/yelp_train.csv')\n",
    "user_avg = read_user('./data/yelp_train.csv')\n",
    "# test_data = read_test_data(train_data, './data/yelp_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vec': {'fThrN4tfupIGetkrz18JOg': '3.0',\n",
       "  '_OimUEl0DzPg6GtJXYeoJw': '3.0',\n",
       "  'RqwuaHAybTqbkmGe1RZLXQ': '4.0',\n",
       "  'WHlbUb5SNdXDKYfkNDo89A': '4.0',\n",
       "  'zlZQM-cJPVW7FHJsYTvyYg': '3.0',\n",
       "  'ZTVWHEAl-1jA_pLyL21s4g': '4.0',\n",
       "  'as_fyujgsdwtV56n31z-Sw': '3.0',\n",
       "  'meXjqyhTNLFmknY39y2sMg': '2.0',\n",
       "  'IqXDpeNbePsW1jR7SSO57w': '2.0',\n",
       "  'tsx0bZbS4ciySrHZr8EFmw': '4.0',\n",
       "  'Bd13s20et9wY2sL72lgWow': '2.0',\n",
       "  'NN6HpdvQlGZUVjBwzhgUWQ': '3.0',\n",
       "  '-BBqiQQCweN7ePwTlY163Q': '4.0',\n",
       "  'V3A7tvyfSX9pmy6L2ohqDg': '3.0',\n",
       "  'MBpTl3womA9RRX4ruce78w': '4.0',\n",
       "  'ZLVWlYQQxzZoJbg4w0gsMA': '4.0',\n",
       "  's1k42BjFaV47wXzh1Ub5ZA': '3.0',\n",
       "  'wT7Y-dJxyKIME09o37DE0A': '4.0',\n",
       "  '0qsarzXaZau8REdE0rToxA': '3.0',\n",
       "  'gNER9lE1Ma9FLm9MBsvYgg': '4.0',\n",
       "  'mlAUV9SKTENbzhWTdl4l4w': '5.0',\n",
       "  'rcoAajAVBBXsBNfgKYaH-w': '4.0',\n",
       "  'MN2oRaV4bZEbBIy_w7fNEQ': '2.0',\n",
       "  'hiceY83htPGn513-IcYWVA': '4.0',\n",
       "  '7D-ATDP9Ezr1ZehszsotOQ': '4.0',\n",
       "  'LNsZJP6jZ11e0tDljOLPiQ': '3.0',\n",
       "  'DBGf3gaJ30ObzQ5fo1SSLg': '3.0',\n",
       "  'kROYR_6mVfg9WgJ7z1HOSA': '4.0',\n",
       "  'T_CniZSYWvVXkCO2hkU5vg': '2.0',\n",
       "  '60AtEye1LKFy284jWFLhpQ': '4.0',\n",
       "  'ZiuGQHIn8NW1NztmwW98uw': '4.0',\n",
       "  '_4qndNOqaB-dDIPCI2MkCg': '4.0',\n",
       "  'wmstf9dw0-kN3YThIxx8eQ': '3.0',\n",
       "  'C3GUi1zAtJdDBWrPoPZjBQ': '5.0',\n",
       "  'GnJwQObGXwR7Sly6Bul0EQ': '4.0',\n",
       "  'sWucpMb_MDdqAthWFFisow': '4.0',\n",
       "  'iqxB3ZreU5GjaVLFhxTdBQ': '3.0',\n",
       "  'EDjz3N5KpsygQxnWi9iP3w': '4.0',\n",
       "  'C29XMB2qx9QT4qix2vvevw': '5.0',\n",
       "  'tXQQ2_fbF7d4FKKODrJFCQ': '4.0',\n",
       "  'TKaND6oV1n2ylnCYoAfoag': '4.0',\n",
       "  'n0NH0FW8NU3zfeJY3QluMQ': '4.0',\n",
       "  'bIA8SE0KHRjzh5WmN8iEkg': '1.0',\n",
       "  'ZWq8Fq13JPDAqt7rPjSwcQ': '4.0',\n",
       "  'BPyopyEntbl4obte91gNrQ': '4.0',\n",
       "  'fNAdDV7HCYnq6P69UGl7kg': '4.0',\n",
       "  'sACSJpBOQ4zPtjsv9trf4Q': '5.0',\n",
       "  '2jzM7PSeG8OwZ5EvocCpQA': '3.0',\n",
       "  'DcgCr11ve0QP29gKMad7OA': '5.0',\n",
       "  'aAVydRJ23okd1AHi2AS4Mw': '4.0'},\n",
       " 'avg': 3.58}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_avg['IlZKVEmRztQtja-6y3rOaA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vec': {'IlZKVEmRztQtja-6y3rOaA': '3.0',\n",
       "  '6P7o2yz4-B8LZDgoqU_Cgg': '4.0',\n",
       "  'dvwFC5u09dbG_16AeNKBmQ': '4.0',\n",
       "  'ALNUwFXAHXmFeQ9V2gCTfA': '4.0',\n",
       "  'wQiyjBi8q_Q0dWLPSnQUqQ': '5.0',\n",
       "  'PIethXohNwpHCMVCfIqcfg': '4.0',\n",
       "  'PEjS7EYRg1cfHyx3r-w0qg': '3.0',\n",
       "  'CNAEooBIpYLNnnugWceDcg': '4.0',\n",
       "  'tcFmCEOQj8BqZ4VZheKCrw': '4.0',\n",
       "  'QaN-nccbLZPWzownQYgTVQ': '4.0',\n",
       "  'ncCeaGkpEQMgzSfBHnatWw': '2.0',\n",
       "  'alxetHC3mXR2PtG8CeCN6Q': '4.0'},\n",
       " 'avg': 3.75}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['fThrN4tfupIGetkrz18JOg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wf1GqnKQuvH-V3QN80UOOQ,fThrN4tfupIGetkrz18JOg,4.536925698145147\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pearson_correlation(business1, business2, user_avg):\n",
    "    vec_set1 = set()\n",
    "    b1_sum = 0\n",
    "    for user, rating in business1['vec'].items():\n",
    "        b1_sum += float(rating)\n",
    "        vec_set1.add(user)\n",
    "    \n",
    "    b2_sum = 0\n",
    "    vec_set2 = set()\n",
    "    for user, rating in business2['vec'].items():\n",
    "        b2_sum += float(rating)\n",
    "        vec_set2.add(user)\n",
    "            \n",
    "    vec_set = vec_set1.union(vec_set2) \n",
    "    if(len(vec_set) == 0):\n",
    "        return 0.0\n",
    "    b1_sum += (len(vec_set)-len(business1['vec'])) * 3.5\n",
    "    b2_sum += (len(vec_set)-len(business2['vec'])) * 3.5\n",
    "    b1_avg = b1_sum/len(vec_set)\n",
    "    b2_avg = b2_sum/len(vec_set)\n",
    "\n",
    "    corr_numerator = 0\n",
    "    corr_denominator_p1 = 0\n",
    "    corr_denominator_p2 = 0\n",
    "    \n",
    "    for user_id in vec_set:\n",
    "        v1 = None\n",
    "        if(user_id in business1['vec']):\n",
    "            v1 = float(business1['vec'][user_id])-b1_avg\n",
    "        else:\n",
    "            v1 = business1['avg']-b1_avg\n",
    "            \n",
    "            \n",
    "        if(user_id in business2['vec']):\n",
    "            v2 = float(business2['vec'][user_id])-b2_avg\n",
    "        else:\n",
    "            v2 = business2['avg']-b2_avg\n",
    "        \n",
    "        corr_numerator += v1*v2\n",
    "        corr_denominator_p1 += math.pow(v1, 2)\n",
    "        corr_denominator_p2 += math.pow(v2, 2)\n",
    "    \n",
    "    denominator = math.pow(corr_denominator_p1, 0.5) * math.pow(corr_denominator_p2, 0.5)\n",
    "    \n",
    "    if(denominator != 0):\n",
    "        return corr_numerator / denominator\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rating(train_data, user_avg, business_id, user_id):\n",
    "    if(not business_id in train_data and not user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, 4.0)\n",
    "    if(not business_id in train_data and user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, user_avg[user_id])\n",
    "    if(business_id in train_data and not user_id in user_avg):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "    \n",
    "    business_data = train_data[business_id]\n",
    "    similar_items = []\n",
    "    for bid, b in train_data.items():\n",
    "        if(user_id in b['vec']):\n",
    "            sim = pearson_correlation(business_data, b, user_avg)\n",
    "            if(sim > 0.0):\n",
    "                similar_items.append((sim, float(b['vec'][user_id])))\n",
    "#     similar_item = sorted(similar_items, key=lambda x: x[0], reverse=True)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for similarity, rating in similar_items:\n",
    "        weight = similarity# * math.pow(abs(similarity), rho-1)\n",
    "        numerator += weight*rating\n",
    "        denominator += weight\n",
    "        \n",
    "    if(denominator != 0):\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, numerator/denominator)\n",
    "    else:\n",
    "        return \"{},{},{}\\n\".format(user_id, business_id, train_data[business_id]['avg'])\n",
    "\n",
    "\n",
    "calculate_rating(train_data, user_avg, 'fThrN4tfupIGetkrz18JOg', 'wf1GqnKQuvH-V3QN80UOOQ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.02546644210815\n"
     ]
    }
   ],
   "source": [
    "def save_output(output, path):\n",
    "    output_file = open(path, 'wt')\n",
    "    output_file.write('user_id, business_id, prediction\\n')\n",
    "    for line in output:\n",
    "        output_file.write(line)\n",
    "    output_file.close()\n",
    "    return\n",
    "    \n",
    "def read_test_data(test_path):\n",
    "    rdd = sc.textFile(test_path)\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda x: x != header).map(read_csv_line).map(lambda x: (x[1], x[0]))#.join(train_data).map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "    \n",
    "    return rdd.collect()\n",
    "\n",
    "st = time.time()\n",
    "test_data = read_test_data('./data/yelp_val.csv')\n",
    "output = sc.parallelize(test_data).map(lambda x: calculate_rating(train_data, user_avg, x[0], x[1])).collect()\n",
    "print(time.time()-st)\n",
    "save_output(output, 'output2-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0634362508560673"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse(test_path, pred_path):\n",
    "    test_data = {}\n",
    "    count = 0\n",
    "    with open(test_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            count+=1\n",
    "            test_data[tuple(line.split(',')[:2])] = float(line.split(',')[2])\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    sum_error_square = 0\n",
    "    with open(pred_path, 'rt') as file:\n",
    "        line = file.readline().strip()\n",
    "        line = file.readline().strip()\n",
    "        while(line):\n",
    "            if(line == 'Null'):\n",
    "                line = file.readline().strip()\n",
    "                continue\n",
    "            sum_error_square+=math.pow((test_data[tuple(line.split(',')[:2])]-float(line.split(',')[2])), 2)\n",
    "            line = file.readline().strip()\n",
    "            \n",
    "    return math.pow(sum_error_square/count, 0.5)\n",
    "\n",
    "rmse('./data/yelp_val.csv', './output2-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.take(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = read_business('./data/yelp_train.csv').filter(lambda x: x[0] == 'cYwJA2A6I12KNkm2rtXd5g').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.parallelize(nearest_neighbor_items).flatMap(lambda x: stage1(x, test_data[1][1][0])).reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest_neighbor_items[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_n = 100\n",
    "\n",
    "# nearest_neighbor_items = read_business('./data/yelp_train.csv').filter(lambda x: is_rated(x, 'wf1GqnKQuvH-V3QN80UOOQ')).map(lambda x: (x[0], pearson_correlation(x, b[0]), x[1])).filter(lambda x: x[1] > 0).collect()#.takeOrdered(mod_n, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_business('./data/yelp_train.csv')\n",
    "test_data = read_test_data(train_data, './data/yelp_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = test_data.cartesian(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rating(vec, user):\n",
    "#     for u, r in vec:\n",
    "#         if(u == user):\n",
    "#             return r\n",
    "#     return 0.0\n",
    "\n",
    "# st = time.time()\n",
    "# x = cp.repartition(16).\\\n",
    "#     map(lambda x: (x[0][0], (x[1][0], pearson_correlation(x[0][2], x[1][1])))).collect()\n",
    "# #     filter(lambda x: x[1][1]>0).collect()#.repartition(16).groupByKey().take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fThrN4tfupIGetkrz18JOg', 'wf1GqnKQuvH-V3QN80UOOQ', {'vec': [('IlZKVEmRztQtja-6y3rOaA', '3.0'), ('6P7o2yz4-B8LZDgoqU_Cgg', '4.0'), ('dvwFC5u09dbG_16AeNKBmQ', '4.0'), ('ALNUwFXAHXmFeQ9V2gCTfA', '4.0'), ('wQiyjBi8q_Q0dWLPSnQUqQ', '5.0'), ('PIethXohNwpHCMVCfIqcfg', '4.0'), ('PEjS7EYRg1cfHyx3r-w0qg', '3.0'), ('CNAEooBIpYLNnnugWceDcg', '4.0'), ('tcFmCEOQj8BqZ4VZheKCrw', '4.0'), ('QaN-nccbLZPWzownQYgTVQ', '4.0'), ('ncCeaGkpEQMgzSfBHnatWw', '2.0'), ('alxetHC3mXR2PtG8CeCN6Q', '4.0')], 'avg': 3.75}), ('fThrN4tfupIGetkrz18JOg', 'EtxsD-Jbyxh7TsaWHtGCew', {'vec': [('IlZKVEmRztQtja-6y3rOaA', '3.0'), ('6P7o2yz4-B8LZDgoqU_Cgg', '4.0'), ('dvwFC5u09dbG_16AeNKBmQ', '4.0'), ('ALNUwFXAHXmFeQ9V2gCTfA', '4.0'), ('wQiyjBi8q_Q0dWLPSnQUqQ', '5.0'), ('PIethXohNwpHCMVCfIqcfg', '4.0'), ('PEjS7EYRg1cfHyx3r-w0qg', '3.0'), ('CNAEooBIpYLNnnugWceDcg', '4.0'), ('tcFmCEOQj8BqZ4VZheKCrw', '4.0'), ('QaN-nccbLZPWzownQYgTVQ', '4.0'), ('ncCeaGkpEQMgzSfBHnatWw', '2.0'), ('alxetHC3mXR2PtG8CeCN6Q', '4.0')], 'avg': 3.75})]\n",
      "['w,fThrN4tfupIGetkrz18JOg,0.0\\n']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ec8b52c007ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/yelp_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data/yelp_val.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output2-1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# st = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ec8b52c007ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_path, test_path, output_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#             line = file.readline().strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_file' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_rating(train_data, business, user_id):\n",
    "    nearest_neighbor_items = train_data.filter(lambda x: is_rated(x[1]['vec'], user_id)).map(lambda x: (x[0], pearson_correlation(x[1], business), x[1])).filter(lambda x: x[1] > 0).collect()#.takeOrdered(5, lambda x: -x[1])\n",
    "    if(len(nearest_neighbor_items) == 0):\n",
    "        return 0.0\n",
    "    n, d = sc.parallelize(nearest_neighbor_items).flatMap(lambda x: stage1(x, user_id)).reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    return n/d \n",
    "\n",
    "def main(train_path, test_path, output_path):\n",
    "    train_data = read_business(train_path).persist()\n",
    "    test_data = read_test_data(train_data, test_path).collect()\n",
    "    output = []\n",
    "    print(test_data[:2])\n",
    "    for item in test_data:\n",
    "        rating_pred = calculate_rating(train_data, item[1][1], item[1][0])\n",
    "        output.append(\"{},{},{}\\n\".format(item[1][0], item[0], rating_pred))\n",
    "        break\n",
    "    print(output)\n",
    "#     with open(test_path, 'rt') as file:\n",
    "#         line = file.readline().strip()\n",
    "#         line = file.readline().strip()\n",
    "#         while(line):\n",
    "#             user, business, rating = line.split(',')\n",
    "#             rating_pred = calculate_rating(train_data, business.strip(), user.strip())\n",
    "#             print(rating_pred)\n",
    "#             output_file.write(\"{},{},{}\\n\".format(user, business, rating_pred))\n",
    "#             line = file.readline().strip()\n",
    "    return\n",
    "\n",
    "# main('./data/yelp_train.csv', './data/yelp_val.csv', 'output2-1.csv')\n",
    "\n",
    "st = time.time()\n",
    "# train_data = train_data.map(lambda x: (x[0], x[0][0], x[0][1])).persist()\n",
    "print()\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
